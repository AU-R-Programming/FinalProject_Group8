---
title: "FinalGroup8 Vignette"
author: "Group 8"
date: "2024-12-02"
output: 
  html_document:
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Installing the Package from GitHub

The first step to using this package is to install the package from github. To do this, we need to install the devtools package `install.packages(devtools)`. Then we use the following code to download the package from github.

```{r message=FALSE, warning=FALSE}
library(devtools)
install_github("AU-R-Programming/FinalProject_Group8")
library(FinalGroup8)
```

Now, the package is available for use.

# Data Pre-Processing

We will demonstrate how to use this function with the `bank.csv` data, which can be found at https://archive.ics.uci.edu/dataset/222/bank+marketing. First, we need to install the data.

```{r}
setwd("~/Documents/GitHub/FinalProject_Group8")
bank <- read.csv("bank.csv", sep = ";")
head(bank)
```

As you can see, the bank data contains 17 columns (16 predictors and 1 response variable). In order to use the `logistic_regression` function in the `FinalGroup8` package, we must prepare the data. First, we need to create a data frame that contains only the x (independent) variables.

```{r message=FALSE, warning=FALSE}
x <- bank[,1:16]
```

Then, we need to create a vector that contains the y (dependent) variable. We also *must* use the `as.factor` function to redefine the y-variable as a factor.

```{r message=FALSE, warning=FALSE}
y <- as.factor(bank[,17])
```

# Using the FinalGroup8 Package for Logistic Regression (for Binary Classification)

Now, the data has been prepared, and we can utilize the `logistic_regression` function. We run the function and store the results in a list that we name `bank_model`.

```{r message=FALSE, warning=FALSE}
bank_model <- logistic_regression(x, y, B = 20, conf_level = 0.95)
```

## Function Output: Coefficient Estimates

Now, we can extract all the elements that were created by running the `logistic_regression` function. First, we can look at the estimated regression coefficients.

```{r message=FALSE, warning=FALSE}
bank_model$coefficient_estimates
```

As you can see, there are a lot of coefficients in the logistic regression model. This is due to the fact that the data used (the bank data) has nine categorical variables. Each of these categorical variables are converted to dummy variables, which lead to a large amount of coefficients.

## Function Output: Bootstrap Intervals

Then, we can compute 95% confidence intervals for the coefficients.

```{r}
bank_model$bootstrap_intervals
```

For our bootstrap intervals, the default parameter values were used (`B = 20` and `conf_level = 0.95`), which used 20 bootstrap samples to create 95% bootstrap confidence intervals for each coefficient. 

For each of the 20 bootstrap samples, a random sample of the original data rows were taken with replacement. Then, using this random sample of data rows, coefficients for each variable were estimated. This was done 20 times to get 20 different estimates for each coefficient. The intervals were then constructed by taking the 2.5th percentile and 97.5th percentile of the 20 values for each coefficient.

## Function Output: Confusion Matrix

The next output created by the `logistic_regression` function is a confusion matrix. The confusion matrix for the bank data is shown below.

```{r}
bank_model$confusion_matrix
```

This confusion matrix gives an estimate on how accurate your logistic regression function is at predicting the true class. It uses the estimated coefficients to predict the class for each data observation.

##Function Output: Confusion Metrics

Finally, our function allows you to look at specific metrics based on the confusion matrix. These metrics are prevalence, accuracy, sensitivity, specificity, false discovery rate, and diagnostic odds ratio.

```{r}
bank_model$confusion_metrics
```

- The prevalence is the proportion of observations in the positive class in the data set. In our example, it refers to the proportion of observations in the data set that are in the "yes" class. 
- The accuracy is simply the proportion of the data that our logistic regression model classifies correctly.
- The sensitivity is the true positive rate. This is the proportion of positive observations that were correctly classified as positive.
- The specificity is the true negative rate. This is the proportion of negative observations that were correctly classified as negative.
- The false discovery rate is the proportion of your positive predictions that were actually negative.
- The diagnostic odds ratio is the odds of predicting a positive observation given the subject is truly in the positive class relative to the odds of the test being positive given the subject is truly in the negative class. A higher diagnostic odds ratio is better.

Something important to note. When I refer to the "positive" class, I am referring to the class that is in the second column/second row of your confusion matrix (in this case, that is the "yes" class), and when I refer to the "negative" class, I am referring to the class that is in the first column/first row of your confusion matrix (in this case, that is the "no" class).

# References

- https://archive.ics.uci.edu/dataset/222/bank+marketing
